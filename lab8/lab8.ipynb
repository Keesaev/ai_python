{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 23:08:00.847386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-22 23:08:01.853794: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-22 23:08:01.853848: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-22 23:08:01.853853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем полный текст книги в файл ```wonderland.txt``` с обрезанными колонтитулами\n",
    "\n",
    "Считываем файл, приводим все символы к нижнему регистру и приводим их к числовым значениям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просуммируем набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144572\n",
      "Total Vocab:  49\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим весь текст на последовательности длиной в 100 символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144472\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем входную последовательность к форме [образцы, временные шаги, особенности] и преобразуем символы в одну кодировку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем LSTM сеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 23:09:02.698779: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-22 23:09:02.698813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (keesaev-asus-ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-12-22 23:09:02.699218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим колбэк Checkpoint для сохранения весов на каждой эпохе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.9920\n",
      "Epoch 1: loss improved from inf to 2.99203, saving model to weights-improvement-01-2.9920.hdf5\n",
      "1129/1129 [==============================] - 98s 87ms/step - loss: 2.9920\n",
      "Epoch 2/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.7983\n",
      "Epoch 2: loss improved from 2.99203 to 2.79834, saving model to weights-improvement-02-2.7983.hdf5\n",
      "1129/1129 [==============================] - 105s 93ms/step - loss: 2.7983\n",
      "Epoch 3/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.7052\n",
      "Epoch 3: loss improved from 2.79834 to 2.70520, saving model to weights-improvement-03-2.7052.hdf5\n",
      "1129/1129 [==============================] - 100s 89ms/step - loss: 2.7052\n",
      "Epoch 4/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.6342\n",
      "Epoch 4: loss improved from 2.70520 to 2.63421, saving model to weights-improvement-04-2.6342.hdf5\n",
      "1129/1129 [==============================] - 102s 90ms/step - loss: 2.6342\n",
      "Epoch 5/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5724\n",
      "Epoch 5: loss improved from 2.63421 to 2.57243, saving model to weights-improvement-05-2.5724.hdf5\n",
      "1129/1129 [==============================] - 103s 91ms/step - loss: 2.5724\n",
      "Epoch 6/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5131\n",
      "Epoch 6: loss improved from 2.57243 to 2.51306, saving model to weights-improvement-06-2.5131.hdf5\n",
      "1129/1129 [==============================] - 107s 94ms/step - loss: 2.5131\n",
      "Epoch 7/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4626\n",
      "Epoch 7: loss improved from 2.51306 to 2.46262, saving model to weights-improvement-07-2.4626.hdf5\n",
      "1129/1129 [==============================] - 104s 92ms/step - loss: 2.4626\n",
      "Epoch 8/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4124\n",
      "Epoch 8: loss improved from 2.46262 to 2.41238, saving model to weights-improvement-08-2.4124.hdf5\n",
      "1129/1129 [==============================] - 104s 92ms/step - loss: 2.4124\n",
      "Epoch 9/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3643\n",
      "Epoch 9: loss improved from 2.41238 to 2.36432, saving model to weights-improvement-09-2.3643.hdf5\n",
      "1129/1129 [==============================] - 103s 91ms/step - loss: 2.3643\n",
      "Epoch 10/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3213\n",
      "Epoch 10: loss improved from 2.36432 to 2.32126, saving model to weights-improvement-10-2.3213.hdf5\n",
      "1129/1129 [==============================] - 104s 92ms/step - loss: 2.3213\n",
      "Epoch 11/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2827\n",
      "Epoch 11: loss improved from 2.32126 to 2.28266, saving model to weights-improvement-11-2.2827.hdf5\n",
      "1129/1129 [==============================] - 100s 88ms/step - loss: 2.2827\n",
      "Epoch 12/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2437\n",
      "Epoch 12: loss improved from 2.28266 to 2.24372, saving model to weights-improvement-12-2.2437.hdf5\n",
      "1129/1129 [==============================] - 101s 90ms/step - loss: 2.2437\n",
      "Epoch 13/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2055\n",
      "Epoch 13: loss improved from 2.24372 to 2.20546, saving model to weights-improvement-13-2.2055.hdf5\n",
      "1129/1129 [==============================] - 99s 87ms/step - loss: 2.2055\n",
      "Epoch 14/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1694\n",
      "Epoch 14: loss improved from 2.20546 to 2.16945, saving model to weights-improvement-14-2.1694.hdf5\n",
      "1129/1129 [==============================] - 104s 92ms/step - loss: 2.1694\n",
      "Epoch 15/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1353\n",
      "Epoch 15: loss improved from 2.16945 to 2.13526, saving model to weights-improvement-15-2.1353.hdf5\n",
      "1129/1129 [==============================] - 104s 92ms/step - loss: 2.1353\n",
      "Epoch 16/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0998\n",
      "Epoch 16: loss improved from 2.13526 to 2.09981, saving model to weights-improvement-16-2.0998.hdf5\n",
      "1129/1129 [==============================] - 101s 89ms/step - loss: 2.0998\n",
      "Epoch 17/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0704\n",
      "Epoch 17: loss improved from 2.09981 to 2.07037, saving model to weights-improvement-17-2.0704.hdf5\n",
      "1129/1129 [==============================] - 104s 92ms/step - loss: 2.0704\n",
      "Epoch 18/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0396\n",
      "Epoch 18: loss improved from 2.07037 to 2.03957, saving model to weights-improvement-18-2.0396.hdf5\n",
      "1129/1129 [==============================] - 105s 93ms/step - loss: 2.0396\n",
      "Epoch 19/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0148\n",
      "Epoch 19: loss improved from 2.03957 to 2.01484, saving model to weights-improvement-19-2.0148.hdf5\n",
      "1129/1129 [==============================] - 105s 93ms/step - loss: 2.0148\n",
      "Epoch 20/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9882\n",
      "Epoch 20: loss improved from 2.01484 to 1.98825, saving model to weights-improvement-20-1.9882.hdf5\n",
      "1129/1129 [==============================] - 104s 92ms/step - loss: 1.9882\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "with tensorflow.device(\"/GPU:0\"):\n",
    "    model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь для преобразования чисел обратно в символы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" \n",
      "alice said nothing; she had sat down with her face in her hands,\n",
      "wondering if anything would _ever_ \"\n",
      " toee i soon the while rabbit  she hor oo the whrle the was oo the toeer, and she was tooting to the tooe, and the woiee war soi time tith the world beain, and saed to the horphon. and alice was sor oo the woile oo the toede oo the was of the sabbet  she had not the hir  and the qooe turted her haad io a lote toiee \n",
      "the was a little oo too that she was soiee to hiree the was soiee to heree the was soie th the toeer, and she was aoling to the kotk of the courd  and saed to the horphon. and alice was sor oo the woile oo the toede oo the was of the sabbet  she had not the hir  and the qooe turted her haad io a lote toiee \n",
      "the was a little oo too that she was soiee to hiree the was soiee to heree the was soie th the toeer, and she was aoling to the kotk of the courd  and saed to the horphon. and alice was sor oo the woile oo the toede oo the was of the sabbet  she had not the hir  and the qooe turted her haad io a lote toiee \n",
      "the was a little oo too that she was soiee to hiree the was soie\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from sys import stdout\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введём пользовательский ```callback```, в котором определим метод ```on_epoch_end```, в котором в канце каждой эпохи будем загружать чекпоинт, собирать из него модель, и генерировать 100 символов текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"GENERATED TEXT (100 symbols):\")\n",
    "\n",
    "        tmp_model = Sequential([\n",
    "            LSTM(256, input_shape=(X.shape[1], X.shape[2])),\n",
    "            Dropout(0.2),\n",
    "            Dense(y.shape[1], activation='softmax')\n",
    "        ])\n",
    "\n",
    "        filepath = \"last_weights.hdf5\"\n",
    "        tmp_model.load_weights(filepath)\n",
    "        tmp_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        \n",
    "        # pick a random seed\n",
    "        start = numpy.random.randint(0, len(dataX)-1)\n",
    "        pattern = dataX[start]\n",
    "        # generate characters\n",
    "        for i in range(100):\n",
    "            x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "            x = x / float(n_vocab)\n",
    "            prediction = tmp_model.predict(x, verbose=0)\n",
    "            index = numpy.argmax(prediction)\n",
    "            result = int_to_char[index]\n",
    "            seq_in = [int_to_char[value] for value in pattern]\n",
    "            sys.stdout.write(result)\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "        print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 3.0288\n",
      "Epoch 1: loss improved from inf to 3.02881, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      "  ao th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th\n",
      "Done.\n",
      "1129/1129 [==============================] - 114s 100ms/step - loss: 3.0288\n",
      "Epoch 2/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.8299\n",
      "Epoch 2: loss improved from 3.02881 to 2.82990, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      " toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe\n",
      "Done.\n",
      "1129/1129 [==============================] - 116s 102ms/step - loss: 2.8299\n",
      "Epoch 3/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.7254\n",
      "Epoch 3: loss improved from 2.82990 to 2.72543, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      " toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe\n",
      "Done.\n",
      "1129/1129 [==============================] - 122s 109ms/step - loss: 2.7254\n",
      "Epoch 4/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.6496\n",
      "Epoch 4: loss improved from 2.72543 to 2.64959, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      " to the toet to the toet to the toet to the tooee to the tooee to the tooee to the tooee to the tooe\n",
      "Done.\n",
      "1129/1129 [==============================] - 118s 104ms/step - loss: 2.6496\n",
      "Epoch 5/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5815\n",
      "Epoch 5: loss improved from 2.64959 to 2.58146, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      "oe woue toe toee  and the woue toe toee  and the woee  the woue toe woee  the woue toe woee  the wou\n",
      "Done.\n",
      "1129/1129 [==============================] - 115s 102ms/step - loss: 2.5815\n",
      "Epoch 6/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5215\n",
      "Epoch 6: loss improved from 2.58146 to 2.52147, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      " oh the sooee  and she sooee to the sooee to the sast on the sooee  and she sooee to the sast oo the\n",
      "Done.\n",
      "1129/1129 [==============================] - 116s 102ms/step - loss: 2.5215\n",
      "Epoch 7/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4632\n",
      "Epoch 7: loss improved from 2.52147 to 2.46319, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      "tou doon  the kart oo the toen                                                                      \n",
      "Done.\n",
      "1129/1129 [==============================] - 117s 104ms/step - loss: 2.4632\n",
      "Epoch 8/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4115\n",
      "Epoch 8: loss improved from 2.46319 to 2.41152, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      " the wast oa the saste  and the sooee was oo tas an the wast oa the saste  and the sooee was oo tas \n",
      "Done.\n",
      "1129/1129 [==============================] - 114s 101ms/step - loss: 2.4115\n",
      "Epoch 9/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3606\n",
      "Epoch 9: loss improved from 2.41152 to 2.36058, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      " couro so the tooed  “ho  the more  fh a loee turtle ”hu an an a lott oa the coure  a datt rai toe t\n",
      "Done.\n",
      "1129/1129 [==============================] - 109s 97ms/step - loss: 2.3606\n",
      "Epoch 10/10\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3163\n",
      "Epoch 10: loss improved from 2.36058 to 2.31634, saving model to last_weights.hdf5\n",
      "GENERATED TEXT (100 symbols):\n",
      "and the more  fn wou den  no ho wou doe toe toene ”ou  a datter wi toen ”hu  toe koow ”hu  a aate pa\n",
      "Done.\n",
      "1129/1129 [==============================] - 109s 96ms/step - loss: 2.3163\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "filepath = \"last_weights.hdf5\"\n",
    "\n",
    "model2 = Sequential([\n",
    "    LSTM(256, input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    Dense(y.shape[1], activation='softmax')\n",
    "])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "tensorboard_callback = TensorBoard(histogram_freq=1)\n",
    "\n",
    "hist = model2.fit(X, y, epochs=10, batch_size=128, callbacks=[checkpoint, CustomCallback(), tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5fcab168657a613eb4a3af5a6ba1dc960dbaab7b10a3c0763673d73c7965562"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
